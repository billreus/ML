# 线性代数

## 范数
有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用被称为范数(norm) 的函数衡量矩阵大小。
Lp 范数如下：

$\lVert x \rVert _p=\left( \sum_i{\left| x_i \right|^p} \right) ^{\frac{1}{p}}$


L1范数 $\lVert x \rVert$ :为x向量各个元素绝对值之和

L2范数$\lVert x \rVert _2$ ：为x向量各个元素平方和的开方

## 特征分解
许多数学对象可以通过将它们分解成多个组成部分。特征分解是使用最广的矩阵分解之一，即将矩阵分解成一组特征向量和特征值。

方阵A的特征向量是指与A相乘后相当于对该向量进行缩放的非零向量$\nu$：

$A\nu \ =\ \lambda \nu$

标量$\lambda$ 被称为这个特征向量对应的特征值。

使用特征分解去分析矩阵A时，得到特征向量构成的矩阵V和特征值构成的向量$\lambda$ ，我们可以重新将A写作：

$A=Vdiag\left( \lambda \right) V^{-1}$

## 奇异值分解（SVD）
矩阵的特征分解是有前提条件的，那就是只有对可对角化的矩阵才可以进行特征分解。但实际中很多矩阵往往不满足这一条件，甚至很多矩阵都不是方阵，就是说连矩阵行和列的数目都不相等。这时候怎么办呢？人们将矩阵的特征分解进行推广，得到了一种叫作“矩阵的奇异值分解”的方法，简称SVD。通过奇异分解，我们会得到一些类似于特征分解的信息。

它的具体做法是将一个普通矩阵分解为奇异向量和奇异值。比如将矩阵A分解成三个矩阵的乘积：

$A=UDV^{T}$ 

假设A是一个m$\times$ n矩阵，那么U是一个m$\times$ m矩阵，D是一个m$\times$ n矩阵，V是一个n$\times$ n矩阵。

这些矩阵每一个都拥有特殊的结构，其中U和V都是正交矩阵，D是对角矩阵（注意，D不一定是方阵）。对角矩阵D对角线上的元素被称为矩阵A的奇异值。矩阵U的列向量被称为左奇异向量，矩阵V 的列向量被称右奇异向量。

SVD最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。另外，SVD可用于推荐系统中。

# 概率

## 条件概率
很多情况下，我们感兴趣的是某个事件在给定其它事件发生时出现的概率，这种概率叫条件概率。

我们将给定X=x时Y=y发生的概率记为$P\left( Y=y|X=x \right)$ ，这个概率可以通过下面的公式来计算：

$P\left( Y=y|X=x \right) =\frac{P\left( Y=y,X=x \right) }{P\left( X=x \right) }$

## 贝叶斯公式
先看看什么是“先验概率”和“后验概率”，以一个例子来说明：

假设某种病在人群中的发病率是0.001，即1000人中大概会有1个人得病，则有： P(患病) = 0.1%；即：在没有做检验之前，我们预计的患病率为P(患病)=0.1%，这个就叫作"先验概率"。

再假设现在有一种该病的检测方法，其检测的准确率为95%；即：如果真的得了这种病，该检测法有95%的概率会检测出阳性，但也有5%的概率检测出阴性；或者反过来说，但如果没有得病，采用该方法有95%的概率检测出阴性，但也有5%的概率检测为阳性。用概率条件概率表示即为：P(显示阳性|患病)=95%

现在我们想知道的是：在做完检测显示为阳性后，某人的患病率P(患病|显示阳性)，这个其实就称为"后验概率"。

而这个叫贝叶斯的人其实就是为我们提供了一种可以利用先验概率计算后验概率的方法，我们将其称为“贝叶斯公式”。

这里先了解条件概率公式：

$P\left( B|A \right)=\frac{P\left( AB \right)}{P\left( A \right)} , P\left( A|B \right)=\frac{P\left( AB \right)}{P\left( B \right)}$

由条件概率可以得到乘法公式：

$P\left( AB \right)=P\left( B|A \right)P\left( A \right)=P\left( A|B \right)P\left( B \right)$

将条件概率公式和乘法公式结合可以得到：

$P\left( B|A \right)=\frac{P\left( A|B \right)\cdot P\left( B \right)}{P\left( A \right)}$

再由全概率公式：

$P\left( A \right)=\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)}$ 

代入可以得到贝叶斯公式：

$P\left( B_{i}|A \right)=\frac{P\left( A|B_{i} \right)\cdot P\left( B_{i} \right)}{\sum_{i=1}^{N}{P\left( A|B_{i} \right) \cdot P\left( B_{i}\right)} }$


贝叶斯公式贯穿了机器学习中随机问题分析的全过程。从文本分类到概率图模型，其基本分类都是贝叶斯公式。

## 期望
在概率论和统计学中，数学期望是试验中每次可能结果的概率乘以其结果的总和。它是最基本的数学特征之一，反映随机变量平均值的大小。

假设X是一个离散随机变量，其可能的取值有：$\left\{ x_{1} ,x_{2} ,......,x_{n} \right\}$ ，各个取值对应的概率取值为：$P\left( x_{k} \right) , k=1,2,......,n$，则其数学期望被定义为：

$E\left(X \right) =\sum_{k=1}^{n}{x_{k} P\left( x_{k} \right) }$ 

假设X是一个连续型随机变量，其概率密度函数为$P\left( x \right)$ 则其数学期望被定义为：

$E\left( x \right) =\int_{-\varpi }^{+\varpi } xf\left( x \right) dx$

## 方差
概率中，方差用来衡量随机变量与其数学期望之间的偏离程度；统计中的方差为样本方差，是各个样本数据分别与其平均数之差的平方和的平均数。数学表达式如下： 

$Var\left( x \right) =E\left\{ \left[ x-E\left( x \right) \right] ^{2} \right\} =E\left( x^{2} \right) -\left[ E\left( x \right) \right] ^{2}$ 

## 协方差
在概率论和统计学中，协方差被用于衡量两个随机变量X和Y之间的总体误差。数学定义式为：

$Cov\left( X,Y \right) =E\left[ \left( X-E\left[ X \right] \right) \left( Y-E\left[ Y \right] \right) \right] =E\left[ XY \right] -E\left[ X \right] E\left[ Y \right]$ 

## 拉格朗日乘子
对于一般的求极值问题我们都知道，求导等于0就可以了。但是如果我们不但要求极值，还要求一个满足一定约束条件的极值，那么此时就可以构造Lagrange函数，其实就是把约束项添加到原函数上，然后对构造的新函数求导。

对于一个要求极值的函数$f\left( x,y \right)$ ，图上的蓝圈就是这个函数的等高图，就是说 $f\left( x,y \right) =c_{1} ,c_{2} ,...,c_{n}$ 分别代表不同的数值(每个值代表一圈，等高图)，我要找到一组$\left( x,y \right)$ ，使它的$c_{i}$ 值越大越好，但是这点必须满足约束条件$g\left( x,y \right)$ （在黄线上）
