<!-- TOC -->

- [1.线性模型](#1线性模型)
    - [1.1.广义线性模型](#11广义线性模型)
        - [1.1.1.最小二乘法](#111最小二乘法)
            - [理论](#理论)
            - [Sklearn](#sklearn)
        - [1.1.2.回归损失函数](#112回归损失函数)
            - [线性回归](#线性回归)
            - [岭回归（L2）和Lasso回归(L1)](#岭回归l2和lasso回归l1)
            - [sklearn](#sklearn)
        - [1.1.3. 梯度下降](#113-梯度下降)
    - [1.2.对数线性模型（逻辑回归）](#12对数线性模型逻辑回归)
        - [1.2.1.原理](#121原理)
        - [1.2.2.损失函数](#122损失函数)
        - [1.2.3.梯度下降](#123梯度下降)
            - [基本梯度](#基本梯度)
            - [正则化L1,L2梯度](#正则化l1l2梯度)
            - [sklearn](#sklearn-1)
            - [随机梯度下降](#随机梯度下降)
    - [1.3.PLA（感知机）](#13pla感知机)
    - [1.4.随机梯度下降](#14随机梯度下降)
        - [1.4.1.理论](#141理论)
        - [1.4.2.Sklearn](#142sklearn)
            - [分类](#分类)
            - [回归](#回归)

<!-- /TOC -->

# 1.线性模型
## 1.1.广义线性模型
给定d个属性$X=\left( X_1;X_2;...;X_d \right)$
预测函数$y=w^TX_i+b\ ;\ X=\left( X_1;X_2;...;X_d \right)$
### 1.1.1.最小二乘法
#### 理论
最小二乘法即求解w,b使$E_{\left( w,b \right)}=\sum_{i=1}^m{\left( y_i-wX_i-b \right) ^2}$最小化的过程。
#### Sklearn
LinearRegression 会调用`fit`方法来拟合数组 X， y，并且将线性模型的系数w存储在其成员变量`coef_`中:
```
>>> from sklearn import linear_model
>>> reg = linear_model.LinearRegression()
>>> reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
>>> reg.coef_
array([ 0.5,  0.5])
```
参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression

### 1.1.2.回归损失函数
#### 线性回归
线性回归的损失函数为：$J\left( \theta \right) =\frac{1}{2m}\sum_{i-1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right)}^2$

注：$h\left( \theta \right) =\theta ^TX=w^TX$

#### 岭回归（L2）和Lasso回归(L1)
岭回归与Lasso回归是为了解决线性回归出现的过拟合以及在通过正规方程方法求解θ的过程中出现的x转置乘以x不可逆这两类问题的，这两种回归均通过在损失函数中引入正则化项来达到目的。

岭回归的损失函数为：$J\left( \theta \right) =\frac{1}{2m}\sum_{i-1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right)}^2+\lambda \sum_{j=1}^m{\theta _{j}^{2}}$

Lasso回归的损失函数：$J\left( \theta \right) =\frac{1}{2m}\sum_{i-1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right)}^2+\lambda \sum_{j=1}^m{\left| \theta _j \right|}$

其中λ称为正则化参数，如果λ选取过大，会把所有参数θ均最小化，造成欠拟合，如果λ选取过小，会导致对过拟合问题解决不当。

岭回归与Lasso回归最大的区别在于岭回归引入的是L2范数惩罚项，Lasso回归引入的是L1范数惩罚项，Lasso回归能够使得损失函数中的许多θ均变成0，这点要优于岭回归，因为岭回归是要所有的θ均存在的，这样计算量Lasso回归将远远小于岭回归。

#### sklearn
岭回归
```
>>> from sklearn import linear_model
>>> reg = linear_model.Ridge (alpha = .5)
>>> reg.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) 
Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,
      normalize=False, random_state=None, solver='auto', tol=0.001)
>>> reg.coef_
array([ 0.34545455,  0.34545455])
>>> reg.intercept_ 
0.13636...
```
参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge

Lasso
```
>>> from sklearn import linear_model
>>> reg = linear_model.Lasso(alpha = 0.1)
>>> reg.fit([[0, 0], [1, 1]], [0, 1])
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute=False, random_state=None,
   selection='cyclic', tol=0.0001, warm_start=False)
>>> reg.predict([[1, 1]])
array([ 0.8])
```
参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso
### 1.1.3. 梯度下降
线性回归中梯度下降即对$J\left( \theta \right)$的$\theta$进行偏导迭代$\theta$

$$
\theta _j=\theta _j-\alpha \frac{\partial}{\partial \theta _j}J\left( \theta _j \right)
$$

$$
=\theta _j-\alpha \frac{\partial}{\partial \theta _j}\frac{1}{2m}\sum_{i=1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2}
$$


$$
=\theta _j-\alpha \frac{1}{m}\sum_{i=1}^m{\left(  \right. \left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right) \bullet X_j^{\left( i \right)}}\left. \right)
$$

注：当j=0时，$X_j^{\left( i \right)}=1$

## 1.2.对数线性模型（逻辑回归）
### 1.2.1.原理
为了将数据二分类，在线性回归的基础上使用对数函数：


$$
\left\{ \begin{array}{c}
	z=w^TX\\
	y=\frac{1}{1+e^{-z}}\\
\end{array} \right.
$$


由y的对数函数在正无穷时y为1，负无穷时y为0,零时为0.5，由此可得$y=\frac{1}{1+e^{-w^TX}}$

### 1.2.2.损失函数
逻辑回归的损失函数为：$J\left( \theta \right) =\frac{1}{m}\sum_{i-1}^m{cost \left( h_{\theta}\left( X^{\left( i \right)} \right) ,\ y^{\left( i \right)} \right)}$

其中：


$$
cost \left( h_{\theta}\left( X^{\left( i \right)} \right) ,\,\,y^{\left( i \right)} \right) = \left\{ \begin{array}{c}
	-\log \left( h_{\theta}\left( x \right) \right) \ if\ y=1\\
	-\log \left( 1-h_{\theta}\left( x \right) \right) \ if\ y=0\\
\end{array} \right. 
$$


### 1.2.3.梯度下降
#### 基本梯度
逻辑回归的梯度下降与线性回归类似首先对上式进行化简合并成一个式子：

$$
cost \left( h_{\theta}\left( X \right) ,y \right) =-y\times \log \left( h_{\theta}\left( X \right) \right) -\left( 1-y \right) \times \log \left( 1-h_{\theta}\left( X \right) \right) 
$$


$$
J\left( \theta \right) =-\frac{1}{m}\sum_{i=1}^m{\left[ y^{\left( i \right)}\log \left( h_{\theta}\left( X^{\left( i \right)} \right) \right) +\left( 1-y^{\left( i \right)} \right) \log \left( 1-h_{\theta}\left( X^{\left( i \right)} \right) \right) \right]}
$$


$$
\theta _j=\theta _j-\alpha \frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right)}X^{\left( i \right)}
$$

#### 正则化L1,L2梯度
与线性回归梯度下降正则化一样也有在后添加L1与L2正则化：

L1:

$$
J\left( \theta \right) =-\frac{1}{m}\sum_{i=1}^m{\left[ y^{\left( i \right)}\log \left( h_{\theta}\left( X^{\left( i \right)} \right) \right) +\left( 1-y^{\left( i \right)} \right) \log \left( 1-h_{\theta}\left( X^{\left( i \right)} \right) \right) \right]}+\frac{\lambda}{2m} \sum_{j=1}^m{\left| \theta _j \right|} 
$$

L2:

$$
J\left( \theta \right) =-\frac{1}{m}\sum_{i=1}^m{\left[ y^{\left( i \right)}\log \left( h_{\theta}\left( X^{\left( i \right)} \right) \right) +\left( 1-y^{\left( i \right)} \right) \log \left( 1-h_{\theta}\left( X^{\left( i \right)} \right) \right) \right]}+\frac{\lambda}{2m} \sum_{j=1}^m{\theta _{j}^{2}} 
$$

#### sklearn

具体参考官方文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression

理论参考中文文档1.1.11.逻辑回归：

http://sklearn.apachecn.org/cn/0.19.0/modules/linear_model.html#sgd

#### 随机梯度下降
随机梯度下降是拟合线性模型的一个简单而高效的方法。在样本量（和特征数）很大时尤为有用。 方法 partial_fit 可用于 online learning （在线学习）或基于 out-of-core learning （外存的学习）

SGDClassifier 和 SGDRegressor 分别用于拟合分类问题和回归问题的线性模型，可使用不同的（凸）损失函数，支持不同的罚项。 例如，设定 loss="log" ，则 SGDClassifier 拟合一个逻辑斯蒂回归模型，而 loss="hinge" 拟合线性支持向量机（SVM）。

具体用法后面章节介绍。

## 1.3.PLA（感知机）
PLA是适用于大规模学习的一种简单算法。默认情况下：

* 不需要设置学习率（learning rate）。
* 不需要正则化处理。
* 仅使用错误样本更新模型。

参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron

## 1.4.随机梯度下降
### 1.4.1.理论
随机梯度下降常用于线性分类器的判别式学习，例如(线性) 支持向量机 和 Logistic 回归 。

我们知道梯度下降每一次迭代都需要出所有的梯度，即每一次算n个梯度，进行迭代，而随机梯度下降每次只选一个梯度进行迭代。

* 梯度下降是每回按照迭代点所在等高线法向量方向前进，虽然计算的慢，收敛的慢，但是迭代的轮数比较少，走的很明确，而且最终会收敛到极小值点，凸函数的话会收敛到最小值点。
* 随机梯度下降每回走的就比较随意，迭代轮数比较多，但是有点就是算的快，收敛的快，但是缺点就是，最终收敛的地方，可能不是极小值点，可能收敛（这里我们定义的收敛就是前后两次迭代的差值小于某个上限）在最后一直在极小值附近的点

参考文档：http://sklearn.apachecn.org/cn/0.19.0/modules/sgd.html#id9

### 1.4.2.Sklearn
#### 分类
* Warning： 在拟合模型前，确保你重新排列了（打乱）)你的训练数据，或者在每次迭代后用 shuffle=True 来打乱。

SGDClassifier 类实现了一个简单的随机梯度下降学习例程, 支持不同的 loss functions（损失函数）和 penalties for classification（分类处罚）。

```
>>> from sklearn.linear_model import SGDClassifier
>>> X = [[0., 0.], [1., 1.]]
>>> y = [0, 1]
>>> clf = SGDClassifier(loss="hinge", penalty="l2")
>>> clf.fit(X, y)
SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,
       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,
       shuffle=True, tol=None, verbose=0, warm_start=False)
```

拟合后，我们可以用该模型来预测新值:

```
>>> clf.predict([[2., 2.]])
array([1])
```

SGD 通过训练数据来拟合一个线性模型。成员 coef_ 保存模型参数:

```
>>> clf.coef_
array([[ 9.9...,  9.9...]])
```

成员 intercept_ 保存 intercept（截距） （又称作 offset（偏移）或 bias（偏差））:

```
>>> clf.intercept_
array([-9.9...])
```

具体的 loss function（损失函数） 可以通过 loss 参数来设置。 SGDClassifier 支持以下的 loss functions（损失函数）：

* loss="hinge": (soft-margin) linear Support Vector Machine （（软-间隔）线性支持向量机），
* loss="modified_huber": smoothed hinge loss （平滑的 hinge 损失），
* loss="log": logistic regression （logistic 回归），
and all regression losses below（以及所有的回归损失）。

参考文档：http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier

#### 回归
SGDRegressor 非常适用于有大量训练样本（>10.000)的回归问题，对于其他问题，我们推荐使用 Ridge ，Lasso ，或 ElasticNet 。

具体的损失函数可以通过 loss 参数设置。 SGDRegressor 支持以下的损失函数:

* loss="squared_loss": Ordinary least squares（普通最小二乘法）,
* loss="huber": Huber loss for robust regression（Huber回归）,
* loss="epsilon_insensitive": linear Support Vector Regression（线性支持向量回归）.

参考文档：http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor