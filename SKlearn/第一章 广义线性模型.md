<!-- TOC -->

- [1.线性模型](#1线性模型)
    - [1.1.广义线性模型](#11广义线性模型)
        - [1.1.1.最小二乘法](#111最小二乘法)
            - [理论](#理论)
            - [Sklearn](#sklearn)
        - [1.1.2.回归损失函数](#112回归损失函数)
            - [线性回归](#线性回归)
            - [岭回归（L2）和Lasso回归(L1)](#岭回归l2和lasso回归l1)
            - [sklearn](#sklearn)
        - [1.1.3. 梯度下降](#113-梯度下降)
    - [1.2.对数线性模型（逻辑回归）](#12对数线性模型逻辑回归)
        - [1.2.1.原理](#121原理)
        - [1.2.2.损失函数](#122损失函数)
        - [1.2.3.梯度下降](#123梯度下降)
            - [基本梯度](#基本梯度)
            - [正则化L1,L2梯度](#正则化l1l2梯度)
            - [sklearn](#sklearn-1)
            - [随机梯度下降](#随机梯度下降)
        - [1.2.4.PLA（感知机）](#124pla感知机)

<!-- /TOC -->

# 1.线性模型
## 1.1.广义线性模型
给定d个属性$X=\left( X_1;X_2;...;X_d \right)$
预测函数$y=w^TX_i+b\ ;\ X=\left( X_1;X_2;...;X_d \right)$
### 1.1.1.最小二乘法
#### 理论
最小二乘法即求解w,b使$E_{\left( w,b \right)}=\sum_{i=1}^m{\left( y_i-wX_i-b \right) ^2}$最小化的过程。
#### Sklearn
LinearRegression 会调用`fit`方法来拟合数组 X， y，并且将线性模型的系数w存储在其成员变量`coef_`中:
```
>>> from sklearn import linear_model
>>> reg = linear_model.LinearRegression()
>>> reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
>>> reg.coef_
array([ 0.5,  0.5])
```
参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression

### 1.1.2.回归损失函数
#### 线性回归
线性回归的损失函数为：$J\left( \theta \right) =\frac{1}{2m}\sum_{i-1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right)}^2$

注：$h\left( \theta \right) =\theta ^TX=w^TX$

#### 岭回归（L2）和Lasso回归(L1)
岭回归与Lasso回归是为了解决线性回归出现的过拟合以及在通过正规方程方法求解θ的过程中出现的x转置乘以x不可逆这两类问题的，这两种回归均通过在损失函数中引入正则化项来达到目的。

岭回归的损失函数为：$J\left( \theta \right) =\frac{1}{2m}\sum_{i-1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right)}^2+\lambda \sum_{j=1}^m{\theta _{j}^{2}}$

Lasso回归的损失函数：$J\left( \theta \right) =\frac{1}{2m}\sum_{i-1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right)}^2+\lambda \sum_{j=1}^m{\left| \theta _j \right|}$

其中λ称为正则化参数，如果λ选取过大，会把所有参数θ均最小化，造成欠拟合，如果λ选取过小，会导致对过拟合问题解决不当。

岭回归与Lasso回归最大的区别在于岭回归引入的是L2范数惩罚项，Lasso回归引入的是L1范数惩罚项，Lasso回归能够使得损失函数中的许多θ均变成0，这点要优于岭回归，因为岭回归是要所有的θ均存在的，这样计算量Lasso回归将远远小于岭回归。

#### sklearn
岭回归
```
>>> from sklearn import linear_model
>>> reg = linear_model.Ridge (alpha = .5)
>>> reg.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) 
Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,
      normalize=False, random_state=None, solver='auto', tol=0.001)
>>> reg.coef_
array([ 0.34545455,  0.34545455])
>>> reg.intercept_ 
0.13636...
```
参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge

Lasso
```
>>> from sklearn import linear_model
>>> reg = linear_model.Lasso(alpha = 0.1)
>>> reg.fit([[0, 0], [1, 1]], [0, 1])
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute=False, random_state=None,
   selection='cyclic', tol=0.0001, warm_start=False)
>>> reg.predict([[1, 1]])
array([ 0.8])
```
参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso
### 1.1.3. 梯度下降
线性回归中梯度下降即对$J\left( \theta \right)$的$\theta$进行偏导迭代$\theta$

$$\theta _j=\theta _j-\alpha \frac{\partial}{\partial \theta _j}J\left( \theta _j \right)$$

$$
=\theta _j-\alpha \frac{\partial}{\partial \theta _j}\frac{1}{2m}\sum_{i=1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2}
$$

$$
=\theta _j-\alpha \frac{1}{m}\sum_{i=1}^m{\left(  \right. \left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right) \bullet X_j^{\left( i \right)}}\left. \right)
$$

注：当j=0时，$X_j^{\left( i \right)}=1$

## 1.2.对数线性模型（逻辑回归）
### 1.2.1.原理
为了将数据二分类，在线性回归的基础上使用对数函数：

$$
\left\{ \begin{array}{c}
	z=w^TX\\
	y=\frac{1}{1+e^{-z}}\\
\end{array} \right. 
$$

由y的对数函数在正无穷时y为1，负无穷时y为0,零时为0.5，由此可得$y=\frac{1}{1+e^{-w^TX}}$

### 1.2.2.损失函数
逻辑回归的损失函数为：$J\left( \theta \right) =\frac{1}{m}\sum_{i-1}^m{cost \left( h_{\theta}\left( X^{\left( i \right)} \right) ,\ y^{\left( i \right)} \right)}$

其中：

$$
cost \left( h_{\theta}\left( X^{\left( i \right)} \right) ,\,\,y^{\left( i \right)} \right) =\left\{ \begin{array}{c}
	-\log \left( h_{\theta}\left( x \right) \right) \ if\ y=1\\
	-\log \left( 1-h_{\theta}\left( x \right) \right) \ if\ y=0\\
\end{array} \right. 
$$

### 1.2.3.梯度下降
#### 基本梯度
逻辑回归的梯度下降与线性回归类似首先对上式进行化简合并成一个式子：

$$
cost \left( h_{\theta}\left( X \right) ,y \right) =-y\times \log \left( h_{\theta}\left( X \right) \right) -\left( 1-y \right) \times \log \left( 1-h_{\theta}\left( X \right) \right) 
$$


$$
J\left( \theta \right) =-\frac{1}{m}\sum_{i=1}^m{\left[ y^{\left( i \right)}\log \left( h_{\theta}\left( X^{\left( i \right)} \right) \right) +\left( 1-y^{\left( i \right)} \right) \log \left( 1-h_{\theta}\left( X^{\left( i \right)} \right) \right) \right]}
$$


$$
\theta _j=\theta _j-\alpha \frac{1}{m}\sum_{i=1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right)}X^{\left( i \right)}
$$

#### 正则化L1,L2梯度
与线性回归梯度下降正则化一样也有在后添加L1与L2正则化：

L1:

$$
J\left( \theta \right) =-\frac{1}{m}\sum_{i=1}^m{\left[ y^{\left( i \right)}\log \left( h_{\theta}\left( X^{\left( i \right)} \right) \right) +\left( 1-y^{\left( i \right)} \right) \log \left( 1-h_{\theta}\left( X^{\left( i \right)} \right) \right) \right]}+\frac{\lambda}{2m} \sum_{j=1}^m{\left| \theta _j \right|} 
$$

L2:

$$
J\left( \theta \right) =-\frac{1}{m}\sum_{i=1}^m{\left[ y^{\left( i \right)}\log \left( h_{\theta}\left( X^{\left( i \right)} \right) \right) +\left( 1-y^{\left( i \right)} \right) \log \left( 1-h_{\theta}\left( X^{\left( i \right)} \right) \right) \right]}+\frac{\lambda}{2m} \sum_{j=1}^m{\theta _{j}^{2}} 
$$

#### sklearn

具体参考官方文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression

理论参考中午文档1.1.11.逻辑回归：

http://sklearn.apachecn.org/cn/0.19.0/modules/linear_model.html#sgd

#### 随机梯度下降
随机梯度下降是拟合线性模型的一个简单而高效的方法。在样本量（和特征数）很大时尤为有用。 方法 partial_fit 可用于 online learning （在线学习）或基于 out-of-core learning （外存的学习）

SGDClassifier 和 SGDRegressor 分别用于拟合分类问题和回归问题的线性模型，可使用不同的（凸）损失函数，支持不同的罚项。 例如，设定 loss="log" ，则 SGDClassifier 拟合一个逻辑斯蒂回归模型，而 loss="hinge" 拟合线性支持向量机（SVM）。

具体用法后面章节介绍。

### 1.2.4.PLA（感知机）
PLA是适用于大规模学习的一种简单算法。默认情况下：

* 不需要设置学习率（learning rate）。
* 不需要正则化处理。
* 仅使用错误样本更新模型。

参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron