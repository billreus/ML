# 1.线性模型
## 1.1.广义线性模型
给定d个属性$X=\left( X_1;X_2;...;X_d \right)$
预测函数$y=w^TX_i+b\ ;\ X=\left( X_1;X_2;...;X_d \right)$
### 1.1.1.最小二乘法
#### 理论
最小二乘法即求解w,b使$E_{\left( w,b \right)}=\sum_{i=1}^m{\left( y_i-wX_i-b \right) ^2}$最小化的过程。
#### Sklearn
LinearRegression 会调用`fit`方法来拟合数组 X， y，并且将线性模型的系数w存储在其成员变量`coef_`中:
```
>>> from sklearn import linear_model
>>> reg = linear_model.LinearRegression()
>>> reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
>>> reg.coef_
array([ 0.5,  0.5])
```

### 1.1.2.回归损失函数
#### 线性回归
线性回归的损失函数为：$J\left( \theta \right) =\frac{1}{2m}\sum_{i-1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right)}^2$，注：$h\left( \theta \right) =\theta ^TX=w^TX$

#### 岭回归（L2）和Lasso回归(L1)
岭回归与Lasso回归是为了解决线性回归出现的过拟合以及在通过正规方程方法求解θ的过程中出现的x转置乘以x不可逆这两类问题的，这两种回归均通过在损失函数中引入正则化项来达到目的。

岭回归的损失函数为：$J\left( \theta \right) =\frac{1}{2m}\sum_{i-1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right)}^2+\lambda \sum_{j=1}^n{\theta _{j}^{2}}$

Lasso回归的损失函数：$J\left( \theta \right) =\frac{1}{2m}\sum_{i-1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right)}^2+\lambda \sum_{j=1}^n{\left| \theta _j \right|}$

其中λ称为正则化参数，如果λ选取过大，会把所有参数θ均最小化，造成欠拟合，如果λ选取过小，会导致对过拟合问题解决不当。

岭回归与Lasso回归最大的区别在于岭回归引入的是L2范数惩罚项，Lasso回归引入的是L1范数惩罚项，Lasso回归能够使得损失函数中的许多θ均变成0，这点要优于岭回归，因为岭回归是要所有的θ均存在的，这样计算量Lasso回归将远远小于岭回归。

#### sklearn
岭回归
```
>>> from sklearn import linear_model
>>> reg = linear_model.Ridge (alpha = .5)
>>> reg.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) 
Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,
      normalize=False, random_state=None, solver='auto', tol=0.001)
>>> reg.coef_
array([ 0.34545455,  0.34545455])
>>> reg.intercept_ 
0.13636...
```
Lasso
```
>>> from sklearn import linear_model
>>> reg = linear_model.Lasso(alpha = 0.1)
>>> reg.fit([[0, 0], [1, 1]], [0, 1])
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute=False, random_state=None,
   selection='cyclic', tol=0.0001, warm_start=False)
>>> reg.predict([[1, 1]])
array([ 0.8])
```

### 1.1.3. 梯度下降
线性回归中梯度下降即对$J\left( \theta \right)$的$\theta$进行偏导迭代$\theta$

$\theta _j=\theta _j-\alpha \frac{\partial}{\partial \theta _j}J\left( \theta _j \right)=\theta _j-\alpha \frac{\partial}{\partial \theta _j}\frac{1}{2m}\sum_{i=1}^m{\left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right) ^2}=\theta _j-\alpha \frac{1}{m}\sum_{i=1}^m{\left(  \right. \left( h_{\theta}\left( X^{\left( i \right)} \right) -y^{\left( i \right)} \right) \bullet X_j^{\left( i \right)}}\left. \right)$
注：当j=0时，$X_j^{\left( i \right)}=1$

## 1.2.对数线性模型（逻辑回归）


