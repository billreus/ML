<!-- TOC -->

- [2.支持向量机](#2支持向量机)
    - [2.1.最优化问题](#21最优化问题)
        - [2.1.1.拉格朗日乘子](#211拉格朗日乘子)
        - [2.1.2.拉格朗日对偶](#212拉格朗日对偶)
    - [2.2.支持向量机理论推导](#22支持向量机理论推导)
        - [2.2.1 线性支持向量机](#221-线性支持向量机)
        - [2.2.2. 对偶向量机](#222-对偶向量机)
        - [2.2.3.核函数](#223核函数)
        - [2.2.4. 软间隔](#224-软间隔)
    - [2.3.Sklearn](#23sklearn)
        - [2.3.1.SVC](#231svc)
        - [2.3.2.SVR](#232svr)
        - [2.3.3 核函数](#233-核函数)

<!-- /TOC -->

# 2.支持向量机

支持向量机 (SVMs) 可用于以下监督学习算法分类,回归和异常检测.
支持向量机的优势在于:
* 在高维空间中非常高效.
* 即使在数据维度比样本数量大的情况下仍然有效.

缺点在于:如果特征数量比样本数量大得多,在选择核函数时要避免过拟合。

对于支持向量机的整个推导过程如下：
线性分类器->(最大化分类间隔)->线性可分SVM->(松弛变量，惩罚因子))->线性不可分SVM->(拉格朗日对偶)->对偶SVM->(核函数)->非线性模型->(SMO算法，KKT条件)->最优解

## 2.1.最优化问题

是对一类问题进行最优化求解，主要分为三类。

$$\text{无约束优化：}\min f\left( x \right)$$
$$\text{有等式约束：}\min f\left( x \right) \ \ s.t.\ h_i\left( x \right) =\text{0;}i=\text{1,...,}n$$
$$\text{不等式约束：}\min f\left( x \right) \ \ s.t.\ g_i\left( x \right) \leqslant \text{0;}i=\text{1,...,}n;h_j\left( x \right) =\text{0;}j=\text{1,...,}$$

注：支持向量机主要是第三种情况

### 2.1.1.拉格朗日乘子

对于如下优化问题：

$$\left\{ \begin{array}{c}
	\min f\left( X \right)\\
	g_i\left( X \right) \leqslant \text{0\ }i=\text{1,...,}q\\
	h_i\left( X \right) =\text{0\ }i=\text{1,...,}p\\
\end{array} \right. $$

首先构造拉格朗日乘子函数

$$L\left( X,\lambda ,\mu \right) =f\left( X \right) +\sum_{j=1}^p{\lambda _j}h_j\left( X \right) +\sum_{k=1}^q{\mu _kg_k\left( X \right)}$$

其中：$\lambda _j,\mu _k$称为拉格朗日乘子，最优解为$X ^*$
必须满足的条件如下，KKT条件

$$\begin{cases}
	h_j\left( X^* \right) =0\\
	g_k\left( X^* \right) \leqslant 0\\
	\nabla _XL\left( X^* \right) =0\\
	\lambda _j\ne 0\\
	\mu _k\geqslant 0\\
	\mu _kg_k\left( X^* \right) =0\\
\end{cases}$$

### 2.1.2.拉格朗日对偶

对偶是最求解优化问题的一种手段，它将一个优化问题转化为另外一个更容易求解的问题.

对于如下优化问题

$$\left\{ \begin{array}{c}
	\min f\left( X \right)\\
	g_i\left( X \right) \leqslant \text{0\ }i=\text{1,...,}m\\
	h_i\left( X \right) =\text{0\ }i=\text{1,...,}p\\
\end{array} \right. $$

仿照拉格朗日乘法构造如下广义拉格朗日函数

$$L\left( X,\lambda ,\nu \right) =f\left( X \right) +\sum_{i=1}^m{\lambda _i}g_i\left( X \right) +\sum_{i=1}^p{\nu _ih_i\left( X \right)}$$

同样称$\lambda _i,\nu _i$为拉格朗日乘子。

变量$\lambda _i$必须满足$\lambda _i\geq 0$

接下来将上面问题转化成所谓的原问题形式，其最优解为

$$p^*=\min _X\max _{\lambda ,\nu ,\lambda _i\geq 0}L\left( X,\lambda ,v \right) =\min _X\theta _P\left( X \right) $$

## 2.2.支持向量机理论推导

### 2.2.1 线性支持向量机

给定m个属性$D=\left( (x_1,y_1),(x_2,y_2),...,(x_m,y_m) \right),y_i\epsilon \left\{ -\text{1，}+1 \right\}$

划分超平面$\omega^Tx+b=0$其中$\omega$即超平面的法向量，垂直于超平面，b为面与原点的距离

假设面上两个点$\dot{x},\ddot{x}$,带入超平面中可得$\omega^T(\ddot{x}-\dot{x})=0$

则任意点到超平面的距离为：

$$r=\left| \left( x-\dot{x} \right) \cos \theta \right|=\left| \lVert x-\dot{x} \rVert \bullet \frac{\left( x-\dot{x} \right) \omega}{\lVert x-\dot{x} \rVert \bullet \lVert \omega \rVert} \right|=\frac{\left| \omega ^Tx-\omega ^T\dot{x} \right|}{\lVert \omega \rVert}$$

代入$\omega^T\dot{x}=-b$得

$$r=\frac{\left| \omega ^Tx+b \right|}{\lVert \omega \rVert}$$

又y=+1,-1所以$r=\frac{1}{\lVert \omega \rVert}$，一面两个异类所以距离$r=\frac{2}{\lVert \omega \rVert}$

最后点到面的距离问题转化成了：

$$\left\{ \begin{array}{c}
	\max _{w,b}\frac{2}{\lVert w \rVert}\\
	st.\ y_i\left( w^Tx_i+b \right) \geqslant \text{1,\ }i=\text{1,2,...,}m\\
\end{array} \right. $$

为了转化成最优化问题，把上式进行转换：

$$\left\{ \begin{array}{c}
	\min _{w,b}\frac{1}{2}\lVert w \rVert ^2\\
	st.\ y_i\left( w^Tx_i+b \right) \geqslant \text{1,\ }i=\text{1,2,...,}m\\
\end{array} \right. $$

### 2.2.2. 对偶向量机

对于上式我们可以通过调用现成的凸二次规划软件包来求解，不过借助拉格朗日函数和对偶问题，我们可以将问题更加简化。

构建拉格朗日函数：

$$L\left( w,b,\alpha \right) =\frac{1}{2}\lVert w \rVert ^2+\sum_{i=1}^m{\alpha _i\left( 1-y_i\left( w^Tx_i+b \right) \right)}$$

对$L\left( w,b,\alpha \right)$中w,b求偏导：

$$\left\{ \begin{array}{c}
	\frac{\partial L\left( b,w,\alpha \right)}{\partial b}=0=\sum_{i=1}^m{\alpha _iy_i}\\
	\frac{\partial L\left( b,w,\alpha \right)}{\partial w}=0=0\sum_{i=1}^m{\alpha _iy_ix_i}\\
\end{array} \right. $$

代入拉格朗日函数得

$$\min _{\alpha}\frac{1}{2}\sum_{i=1}^m{\sum_{j=1}^m{\alpha _i\alpha _jy_iy_j}}x_{i}^{T}x_j-\sum_{i=1}^m{\alpha _i}$$

$$s.t.\ \sum_{i=1}^m{\alpha _iy_i=0}$$

$$\alpha _i\geqslant \text{0,\ }i=\text{1,2,...,}m$$

所以线性支持向量机的假设函数可表示为：

回归：

$$f\left( x \right) =w^Tx+b=\sum_{i=1}^m{\alpha _iy_ix_{i}^{T}x+b}$$

分类：

$$f\left( x \right) =w^Tx+b=sign\left( \sum_{i=1}^m{\alpha _iy_ix_{i}^{T}x+b} \right) $$

### 2.2.3.核函数

为了解决非线性分类需要在低纬度进行计算，高维度构建最优分离超平面。

设：$\kappa \left( x_i,x_j \right) =\left< \phi \left( x_i \right) ,\phi \left( x_j \right) \right> =\phi \left( x_i \right) ^T\phi \left( x_j \right)$

代入原函数得：

$$f\left( x \right) =w^T\phi \left( x \right) +b=\sum_{i=1}^m{\alpha _iy_i\phi \left( x_i \right) ^T\phi \left( x_i \right) +b}=\sum_{i=1}^m{\alpha _iy_i\kappa \left( x,x_i \right) +b}$$

一般的常用核函数为：

线性核：$\kappa \left( x_i,x_j \right) =x_{i}^{T}x_j$

多项式核：$\kappa \left( x_i,x_j  \right) =(\gamma x_{i}^{T}x_j +b) ^d$

高斯核(RBF)：$\kappa \left( x_i,x_j \right) =\exp \left( -\gamma \lVert  x_{i}^{T}x_j  \rVert ^2  \right)$

通常, 当特征维数d超过样本数m时(文本分类问题通常是这种情况), 使用线性核; 当特征维数 d 比较小. 样本数 m 中等时, 使用 RBF 核; 当特征维数 d 比较小. 样本数 m 特别大时,支持向量机性能通常不如深度神经网络.

### 2.2.4. 软间隔

有时候划需要允许有些变量在-1到1的范围内，我们希望在优化间隔的同时, 允许分类错误的样本出现, 但这类样本应尽可能少。

$$\min _{w,b}\ \frac{1}{2}w^Tw+C\sum_{i=1}^m{\xi _i}$$

$$\xi _i=\left\{ \begin{array}{c}
	\text{0\ ;\ }y_i\left( w^T\phi \left( x_i \right) +b \right) \geqslant \text{1}\\
	1-y_i\left( w^T\phi \left( x_i \right) +b \right) \ ;\text{其它}\\
\end{array} \right.$$

由于$\xi_i$又称0/1损失不够连续所以常用其他曲线更好的松弛变量如：

铰链损失：max(0, 1-s)常用语支持向量机

对数几率损失：log(1+exp(-s))常用于对数回归

指数损失：exp(-s)常用于AdaBoost

## 2.3.Sklearn

### 2.3.1.SVC

```
>>> from sklearn import svm
>>> X = [[0, 0], [1, 1]]
>>> y = [0, 1]
>>> clf = svm.SVC()
>>> clf.fit(X, y)  
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
```

在拟合后, 这个模型可以用来预测新的值:

```
>>> clf.predict([[2., 2.]])
array([1])
```
SVMs 决策函数取决于训练集的一些子集, 称作支持向量. 这些支持向量的部分特性可以在 support_vectors_, support_ 和 n_support 找到:
```
>>> # 获得支持向量
>>> clf.support_vectors_
array([[ 0.,  0.],
       [ 1.,  1.]])
>>> # 获得支持向量的索引get indices of support vectors
>>> clf.support_ 
array([0, 1]...)
>>> # 为每一个类别获得支持向量的数量
>>> clf.n_support_ 
array([1, 1]...)
```

参考文档：http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC

### 2.3.2.SVR

SVC为分类，SVR用于处理回归：
```
>>> from sklearn import svm
>>> X = [[0, 0], [2, 2]]
>>> y = [0.5, 2.5]
>>> clf = svm.SVR()
>>> clf.fit(X, y) 
SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',
    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
>>> clf.predict([[1, 1]])
array([ 1.5])
```
参考文档：http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR

### 2.3.3 核函数

核函数 可以是以下任何形式：:

* 线性: $\langle x, x'\rangle.$
* 多项式: $(\gamma \langle x, x'\rangle + r)^d.$ d 是关键词 degree, r 指定 coef0。
* rbf: $\exp(-\gamma \|x-x'\|^2).\gamma$ 是关键词 gamma, 必须大于 0。
* sigmoid $(\tanh(\gamma \langle x,x'\rangle + r))$, 其中 r 指定 coef0。
初始化时，不同内核由不同的函数名调用:
```
>>> linear_svc = svm.SVC(kernel='linear')
>>> linear_svc.kernel
'linear'
>>> rbf_svc = svm.SVC(kernel='rbf')
>>> rbf_svc.kernel
'rbf'
```