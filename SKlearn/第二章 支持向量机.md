# 2.支持向量机
支持向量机 (SVMs) 可用于以下监督学习算法分类,回归和异常检测.
支持向量机的优势在于:
* 在高维空间中非常高效.
* 即使在数据维度比样本数量大的情况下仍然有效.

缺点在于:如果特征数量比样本数量大得多,在选择核函数时要避免过拟合。

对于支持向量机的整个推导过程如下：
线性分类器->线性可分SVM->线性不可分SVM->对偶SVM->非线性模型->最优解

## 2.1.最优化问题
是对一类问题进行最优化求解，主要分为三类。

$$\text{无约束优化：}\min f\left( x \right)$$
$$\text{有等式约束：}\min f\left( x \right) \ \ s.t.\ h_i\left( x \right) =\text{0;}i=\text{1,...,}n$$
$$\text{不等式约束：}\min f\left( x \right) \ \ s.t.\ g_i\left( x \right) \leqslant \text{0;}i=\text{1,...,}n;h_j\left( x \right) =\text{0;}j=\text{1,...,}$$

注：支持向量机主要是第三种情况

### 2.1.1.拉格朗日乘子
对于如下优化问题：

$$
\left\{ \begin{array}{c}
	\min f\left( X \right)\\
	g_i\left( X \right) \leqslant \text{0\ }i=\text{1,...,}q\\
	h_i\left( X \right) =\text{0\ }i=\text{1,...,}p\\
\end{array} \right. 
$$


首先构造拉格朗日乘子函数


$$
L\left( X,\lambda ,\mu \right) =f\left( X \right) +\sum_{j=1}^p{\lambda _j}h_j\left( X \right) +\sum_{k=1}^q{\mu _kg_k\left( X \right)}
$$


其中：$\lambda _j,\mu _k$称为拉格朗日乘子，最优解为$X ^*$
必须满足的条件如下，KKT条件

$$
\begin{cases}
	h_j\left( X^* \right) =0\\
	g_k\left( X^* \right) \leqslant 0\\
	\nabla _XL\left( X^* \right) =0\\
	\lambda _j\ne 0\\
	\mu _k\geqslant 0\\
	\mu _kg_k\left( X^* \right) =0\\
\end{cases}
$$

### 2.1.2.拉格朗日对偶
对偶是最求解优化问题的一种手段，它将一个优化问题转化为另外一个更容易求解的问题.

对于如下优化问题

$$
\left\{ \begin{array}{c}
	\min f\left( X \right)\\
	g_i\left( X \right) \leqslant \text{0\ }i=\text{1,...,}m\\
	h_i\left( X \right) =\text{0\ }i=\text{1,...,}p\\
\end{array} \right. 
$$

仿照拉格朗日乘法构造如下广义拉格朗日函数

$$
L\left( X,\lambda ,\nu \right) =f\left( X \right) +\sum_{i=1}^m{\lambda _i}g_i\left( X \right) +\sum_{i=1}^p{\nu _ih_i\left( X \right)}
$$

同样称$\lambda _i,\nu _i$为拉格朗日乘子。

变量$\lambda _i$必须满足$\lambda _i\geq 0$

接下来将上面问题转化成所谓的原问题形式，其最优解为

$$
p^*=\min _X\max _{\lambda ,\nu ,\lambda _i\geq 0}L\left( X,\lambda ,v \right) =\min _X\theta _P\left( X \right) 
$$

## 2.2.支持向量机理论推导
### 2.2.1 基本形式
给定m个属性$D=\left( (x_1,y_1),(x_2,y_2),...,(x_m,y_m) \right),y_i\epsilon \left\{ -\text{1，}+1 \right\}$

划分超平面$\omega^Tx+b=0$其中$\omega$即超平面的法向量，垂直于超平面，b为面与原点的距离

假设面上两个点$\dot{x},\ddot{x}$,带入超平面中可得$\omega^T(\ddot{x}-\dot{x})=0$

则任意点到超平面的距离为：

$$
r=\left| \left( x-\dot{x} \right) \cos \theta \right|=\left| \lVert x-\dot{x} \rVert \bullet \frac{\left( x-\dot{x} \right) \omega}{\lVert x-\dot{x} \rVert \bullet \lVert \omega \rVert} \right|=\frac{\left| \omega ^Tx-\omega ^T\dot{x} \right|}{\lVert \omega \rVert}
$$

代入$\omega^T\dot{x}=-b$得

$$
r=\frac{\left| \omega ^Tx+b \right|}{\lVert \omega \rVert}
$$

又y=+1,-1所以$r=\frac{1}{\lVert \omega \rVert}$，一面两个异类所以距离$r=\frac{2}{\lVert \omega \rVert}$

最后点到面的距离问题转化成了：

$$
\left\{ \begin{array}{c}
	\max _{w,b}\frac{2}{\lVert w \rVert}\\
	st.\ y_i\left( w^Tx_i+b \right) \geqslant \text{1,\ }i=\text{1,2,...,}m\\
\end{array} \right. 
$$

为了转化成最优化问题，把上式进行转换：

$$
\left\{ \begin{array}{c}
	\min _{w,b}\frac{1}{2}\lVert w \rVert ^2\\
	st.\ y_i\left( w^Tx_i+b \right) \geqslant \text{1,\ }i=\text{1,2,...,}m\\
\end{array} \right. 
$$