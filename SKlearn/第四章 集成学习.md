<!-- TOC -->

- [4.集成学习](#4集成学习)
    - [4.1.Bagging（装袋）](#41bagging装袋)
        - [4.1.1.基本原理](#411基本原理)
        - [4.1.2 Sklearn](#412-sklearn)
        - [4.2.1.随机森林](#421随机森林)
            - [工作原理](#工作原理)
            - [Sklearn](#sklearn)
    - [4.2.Boosting](#42boosting)
        - [4.2.1.原理](#421原理)
        - [4.2.2.AdaBoost](#422adaboost)
            - [过程](#过程)
            - [数据抽象](#数据抽象)
        - [4.2.3.Sklearn](#423sklearn)
    - [4.3.GBDT(Gradient Boosting Decision Tree)](#43gbdtgradient-boosting-decision-tree)
        - [4.3.1.原理](#431原理)
        - [4.3.2.Sklearn](#432sklearn)
    - [4.4.XGBOOST](#44xgboost)
        - [4.4.1.铺垫](#441铺垫)
        - [4.4.2.数学模型](#442数学模型)
        - [4.4.3.实用](#443实用)
    - [4.5.模型融合](#45模型融合)
        - [4.5.1.voting](#451voting)
        - [4.5.2.Stacking](#452stacking)
            - [基础理论](#基础理论)
            - [实际应用](#实际应用)
            - [Sklearn](#sklearn-1)

<!-- /TOC -->

# 4.集成学习

集成学习可以分为三大类：
* 用于减少方差的bagging
* 用于减少偏差的boosting
* 用于提升预测结果的stacking

## 4.1.Bagging（装袋）

### 4.1.1.基本原理

每一次从原始数据中根据均匀概率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。

* 1.从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
* 2.每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
* 3.对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

### 4.1.2 Sklearn

在 scikit-learn 中，bagging 方法使用统一的 BaggingClassifier 元估计器（或者 BaggingRegressor ），输入的参数和随机子集抽取策略由用户指定。max_samples 和 max_features 控制着子集的大小（对于样例和特征）， bootstrap 和 bootstrap_features 控制着样例和特征的抽取是有放回还是无放回的。 当使用样本子集时，通过设置 oob_score=True ，可以使用袋外(out-of-bag)样本来评估泛化精度。

下面的代码片段说明了如何构造一个 K聚类 估计器的 bagging 集成实例，每一个基估计器都建立在 50% 的样本随机子集和 50% 的特征随机子集上。
```
>>> from sklearn.ensemble import BaggingClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> bagging = BaggingClassifier(KNeighborsClassifier(),
...                             max_samples=0.5, max_features=0.5)
```

参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier

### 4.2.1.随机森林

#### 工作原理

随机森林算法有两个阶段，一个阶段是创建随机森林，另一个是根据前一个阶段创建的随机森林分类器做出预测。

首先是随机森林创建阶段：

* 1.从全部m个特征中随机选择K个特征，其中k << m
* 2.在K个特征中，用最佳分裂点计算节点d
* 3.用最佳分裂将节点分裂为子节点
* 4.重复前面三步的过程，直到获得I个数量的节点。
* 5.重复第1到第4步n次创建n个树，从而形成一个森林。

在第二阶段，根据上一阶段创建的随机森林分类器，我们会做出预测。过程如下：

* 选取测试特征，用每个随机创建的决策树的规律去预测结果，并保存预测的结果（目标）。
* 结算每个预测目标的得票数。
* 将得票最多的预测目标作为随机森林算法的最终预测。

其实类似于使用多个决策树，数据采用bagging的方法随机，构建多个树，最后以投票的方式决定结果。

随机森林中数据集的划分采用bagging但是在和对于$g_t(x)$的好坏验证上原始方法是bagging到的数据集用于得到g(x)，剩下的数据用于验证g的好坏。随机森林中采用OOB即先看每一个样本是哪些g的OOB，然后计算在这些g上的表现取平均值。

#### Sklearn

森林分类器必须拟合（fit）两个数组： 保存训练样本的数组（或稀疏或稠密的）X，大小为 [n_samples, n_features]，和 保存训练样本目标值（类标签）的数组 Y，大小为 [n_samples]:
```
>>> from sklearn.ensemble import RandomForestClassifier
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = RandomForestClassifier(n_estimators=10)
>>> clf = clf.fit(X, Y)

>>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y)
>>> scores.mean()
0.999...
```
参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor

在极限随机树中（参见 ExtraTreesClassifier 和 ExtraTreesRegressor 类)， 计算分割点方法中的随机性进一步增强。 在随机森林中，使用的特征是候选特征的随机子集；不同于寻找最具有区分度的阈值， 这里的阈值是针对每个候选特征随机生成的，并且选择这些随机生成的阈值中的最佳者作为分割规则。 这种做法通常能够减少一点模型的方差，代价则是略微地增大偏差：
```
>>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y)
>>> scores.mean() > 0.999
True
```

使用这些方法时要调整的参数主要是 n_estimators 和 max_features。 

前者（n_estimators）是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。 此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好。 后者（max_features）是分割节点时考虑的特征的随机子集的大小。 这个值越低，方差减小得越多，但是偏差的增大也越多。 

根据经验，回归问题中使用 max_features = n_features ， 分类问题使用 max_features = sqrt（n_features （其中 n_features 是特征的个数）是比较好的默认值。 max_depth = None 和 min_samples_split = 2 结合通常会有不错的效果（即生成完全的树）。 请记住，这些（默认）值通常不是最佳的，同时还可能消耗大量的内存，最佳参数值应由交叉验证获得。 

另外，请注意，在随机森林中，默认使用自助采样法（bootstrap = True）， 然而 extra-trees 的默认策略是使用整个数据集（bootstrap = False）。 当使用自助采样法方法抽样时，泛化精度是可以通过剩余的或者袋外的样本来估算的，设置 oob_score = True 即可实现。

最后，这个模块还支持树的并行构建和预测结果的并行计算，这可以通过 n_jobs 参数实现。 如果设置 n_jobs = k ，则计算被划分为 k 个作业，并运行在机器的 k 个核上。 如果设置 n_jobs = -1 ，则使用机器的所有核。 注意由于进程间通信具有一定的开销，这里的提速并不是线性的（即，使用 k 个作业不会快 k 倍）。 当然，在建立大量的树，或者构建单个树需要相当长的时间（例如，在大数据集上）时，（通过并行化）仍然可以实现显著的加速。

参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor

## 4.2.Boosting

### 4.2.1.原理

Boosting算法的思想是通过对弱分类器的逐步优化使之成为强分类器。

假设当前训练数据集中存在N个点，其权重分别赋值为$w_i$，在迭代学习中根据结果不断调整权重，如果分类正确，则降低这些点的权重，如果分类错误，则增加权重，这样当迭代结束时，算法将得到m个合适的模型，最后通过加权平均得到最后的预测结果。

boosting的每一次抽样的样本分布都是不一样的。每一次迭代，都根据上一次迭代的结果，增加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个不断学习的过程，也是一个不断提升的过程，这也就是boosting思想的本质所在。迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。

### 4.2.2.AdaBoost

#### 过程

1、初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。

2、训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。

3、将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。

#### 数据抽象

设训练数据集

$T={(x_1, y_1),(x_2, y_2)...(x_N,Y_N)}$

初始化训练数据的权值分布：

$D_1=(w_{11},w_{12},...,w_{1i},...,w_{1N}), w_{1i}=\frac{1}{N}$

根据权重分布抽样产生数据集并学习，得到基本分类器：$G_m(x)$

计算$G_m(x)$在训练数据集上的分类误差率

$$
e_m=P\left( G_m\left( x_i \right) \ne y_i \right) =\sum_{i=1}^N{w_{mi}I\left( G_m\left( x_i \right) \ne y_i \right)}
$$

计算$G_m(x)$的系数

$$
\alpha _m=\frac{1}{2}\log \frac{1-e_m}{e_m}
$$

根据系数开始更新数据，首先更新训练数据集的权值分布

$$
D_{m+1}=(w_{m+1,1},w_{m+1,2},...,w_{m+1,i},...,w_{m+1,N})
$$

$$
w_{m+\text{1,}j}=\frac{w_{mj}}{Z_m}\exp \left( -\alpha _my_iG\left( x_i \right) \right) ,i=\text{1,2,...,}N
$$
* 其中$Z_m$是规范化因子，可以使D概率分布:

$$
Z_m=\sum_{i=1}^N{w_{mi}\exp \left( -\alpha _my_iG_m\left( x_i \right) \right)}
$$

同时$w_{m+1,j}$的更新也可以写成：

$$
w_{m+\text{1,}i}=\left\{ \begin{array}{c}
	\frac{w_{mi}}{Z_m}e^{-\alpha _m},G_m\left( x_i \right) =y_i\\
	\frac{w_{mi}}{Z_m}e^{\alpha _m},G_m\left( x_i \right) \ne y_i\\
\end{array} \right. 
$$

最后对分类器进行线性组合

$$
f\left( x \right) =\sum_{m=1}^M{\alpha _mG_m\left( x \right)}
$$

最终分类器
$G(x)=sign(f(x))$

### 4.2.3.Sklearn

```
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.datasets import load_iris
>>> from sklearn.ensemble import AdaBoostClassifier

>>> iris = load_iris()
>>> clf = AdaBoostClassifier(n_estimators=100)
>>> scores = cross_val_score(clf, iris.data, iris.target)
>>> scores.mean()                             
0.9...
```
弱学习器的数量由参数 n_estimators 来控制。 learning_rate 参数用来控制每个弱学习器对 最终的结果的贡献程度（校对者注：其实应该就是控制每个弱学习器的权重修改速率，这里不太记得了，不确定）。 弱学习器默认使用决策树。不同的弱学习器可以通过参数 base_estimator 来指定。 获取一个好的预测结果主要需要调整的参数是 n_estimators 和 base_estimator 的复杂度 (例如:对于弱学习器为决策树的情况，树的深度 max_depth 或叶子节点的最小样本数 min_samples_leaf 等都是控制树的复杂度的参数)

参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier
http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor

## 4.3.GBDT(Gradient Boosting Decision Tree)

### 4.3.1.原理

GBDT是以决策树（CART）为基学习器的GB算法.也属于boosting。

GBDT的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。

比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。

### 4.3.2.Sklearn

```
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier

>>> X, y = make_hastie_10_2(random_state=0)
>>> X_train, X_test = X[:2000], X[2000:]
>>> y_train, y_test = y[:2000], y[2000:]

>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.913...
```
弱学习器(例如:回归树)的数量由参数 n_estimators 来控制；每个树的大小可以通过由参数 max_depth 设置树的深度，或者由参数 max_leaf_nodes 设置叶子节点数目来控制。 learning_rate 是一个在 (0,1] 之间的超参数，这个参数通过 shrinkage(缩减步长) 来控制过拟合。

参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier

## 4.4.XGBOOST

Xgboost相比于GBDT来说，更加有效应用了数值优化，最重要是对损失函数（预测值和真实值的误差）变得更复杂。目标函数依然是所有树的预测值相加等于预测值。

### 4.4.1.铺垫

分类与回归是两个很接近的问题，分类的目标是根据已知样本的某些特征，判断一个新的样本属于哪种已知的样本类，它的结果是离散值。而回归的结果是连续的值。

### 4.4.2.数学模型

在GBDT的基础上添加了优化函数比较复杂。

### 4.4.3.实用

```
import xgboost as xgb

xgb_model = xgb.XGBClassifier(n_estimators=150,min_samples_leaf=3,max_depth=6)
xgb_model.fit(train_data_X,train_data_Y)

test["Survived"] = xgb_model.predict(test_data_X)
```

## 4.5.模型融合

### 4.5.1.voting

即使用投票的方式对最后结果进行投票，选票数多的为结果。
```
from sklearn.ensemble import VotingClassifier

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C=0.1,max_iter=100)

import xgboost as xgb
xgb_model = xgb.XGBClassifier(max_depth=6,min_samples_leaf=2,n_estimators=100,num_round = 5)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=200,min_samples_leaf=2,max_depth=6,oob_score=True)

from sklearn.ensemble import GradientBoostingClassifier
gbdt = GradientBoostingClassifier(learning_rate=0.1,min_samples_leaf=2,max_depth=6,n_estimators=100)

vot = VotingClassifier(estimators=[('lr', lr), ('rf', rf),('gbdt',gbdt),('xgb',xgb_model)], voting='hard')
vot.fit(train_data_X_sd,train_data_Y)
```

### 4.5.2.Stacking

Stacking是通过一个元分类器或者元回归器来整合多个分类模型或回归模型的集成学习技术。基础模型利用整个训练集做训练，元模型将基础模型的特征作为特征进行训练。

#### 基础理论 

Stacking模型本质上是一种分层的结构，这里简单起见，只分析二级Stacking.

假设我们有3个基模型M1、M2、M3。对模型M1的训练,其中预测的训练集train1和测试集test2标签列为$P_1$和$T_1$,然后对M2,M3,重复得到$P_2,T_2,P_3,T_3$。

分别把$P_1,P_2,P_3$以及$T_1,T_2,T_3$合并，得到新的训练集train2和测试集test2。

再用第二层的模型M4训练train2，预测test2，最终得到标签列。

#### 实际应用

Stacking本质上就是这么直接的思路，但是这样肯定是不行的，问题在于P1的得到是有问题的，用整个训练集训练的模型反过来去预测训练集的标签，毫无疑问过拟合是非常非常严重的，因此现在的问题变成了如何在解决过拟合的前提下得到P1、P2、P3，这就变成了熟悉的节奏——K折交叉验证。

需要将训练集切分，例如分出前二分之一作为训练集traina训练模型m1，然后在后二分之一trainb上预测得到preb3，preb4；再在trainb上训练模型m1,然后在后二分之一trainb上预测得到preb1，preb2。再把两个预测集拼接成P1。

#### Sklearn

```
# 划分train数据集,调用代码,把数据集名字转成和代码一样
X = train_data_X_sd
X_predict = test_data_X_sd
y = train_data_Y

'''模型融合中使用到的各个单模型'''
from sklearn.linear_model import LogisticRegression
from sklearn import svm
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

clfs = [LogisticRegression(C=0.1,max_iter=100),
        xgb.XGBClassifier(max_depth=6,n_estimators=100,num_round = 5),
        RandomForestClassifier(n_estimators=100,max_depth=6,oob_score=True),
        GradientBoostingClassifier(learning_rate=0.3,max_depth=6,n_estimators=100)]

# 创建n_folds(k交叉，5模型)
from sklearn.cross_validation import StratifiedKFold
n_folds = 5
skf = list(StratifiedKFold(y, n_folds))

# 创建零矩阵
dataset_blend_train = np.zeros((X.shape[0], len(clfs)))
dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))

# 建立模型
for j, clf in enumerate(clfs):
    '''依次训练各个单模型'''
    # print(j, clf)
    dataset_blend_test_j = np.zeros((X_predict.shape[0], len(skf)))
    for i, (train, test) in enumerate(skf):
        '''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''
        # print("Fold", i)
        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]
        clf.fit(X_train, y_train)
        y_submission = clf.predict_proba(X_test)[:, 1]
        dataset_blend_train[test, j] = y_submission
        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]
    '''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''
    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)

# 用建立第二层模型
clf2 = LogisticRegression(C=0.1,max_iter=100)
clf2.fit(dataset_blend_train, y)
y_submission = clf2.predict_proba(dataset_blend_test)[:, 1]
```