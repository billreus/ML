# 4.集成学习
集成学习可以分为三大类：
* 用于减少方差的bagging
* 用于减少偏差的boosting
* 用于提升预测结果的stacking

## 4.1 Bagging（装袋）
### 4.1.1.基本原理
* 1.从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
* 2.每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
* 3.对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

### 4.1.2 Sklearn
在 scikit-learn 中，bagging 方法使用统一的 BaggingClassifier 元估计器（或者 BaggingRegressor ），输入的参数和随机子集抽取策略由用户指定。max_samples 和 max_features 控制着子集的大小（对于样例和特征）， bootstrap 和 bootstrap_features 控制着样例和特征的抽取是有放回还是无放回的。 当使用样本子集时，通过设置 oob_score=True ，可以使用袋外(out-of-bag)样本来评估泛化精度。

下面的代码片段说明了如何构造一个 K聚类 估计器的 bagging 集成实例，每一个基估计器都建立在 50% 的样本随机子集和 50% 的特征随机子集上。
```
>>> from sklearn.ensemble import BaggingClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> bagging = BaggingClassifier(KNeighborsClassifier(),
...                             max_samples=0.5, max_features=0.5)
```
参考文档：http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier

### 4.2.1.随机森林