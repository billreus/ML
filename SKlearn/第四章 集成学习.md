# 4.集成学习
集成学习可以分为三大类：
* 用于减少方差的bagging
* 用于减少偏差的boosting
* 用于提升预测结果的stacking

## 4.1.Bagging（装袋）
### 4.1.1.基本原理
* 1.从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
* 2.每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
* 3.对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

### 4.1.2 Sklearn
在 scikit-learn 中，bagging 方法使用统一的 BaggingClassifier 元估计器（或者 BaggingRegressor ），输入的参数和随机子集抽取策略由用户指定。max_samples 和 max_features 控制着子集的大小（对于样例和特征）， bootstrap 和 bootstrap_features 控制着样例和特征的抽取是有放回还是无放回的。 当使用样本子集时，通过设置 oob_score=True ，可以使用袋外(out-of-bag)样本来评估泛化精度。

下面的代码片段说明了如何构造一个 K聚类 估计器的 bagging 集成实例，每一个基估计器都建立在 50% 的样本随机子集和 50% 的特征随机子集上。
```
>>> from sklearn.ensemble import BaggingClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> bagging = BaggingClassifier(KNeighborsClassifier(),
...                             max_samples=0.5, max_features=0.5)
```
参考文档：http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier

### 4.2.1.随机森林
#### 工作原理
随机森林算法有两个阶段，一个阶段是创建随机森林，另一个是根据前一个阶段创建的随机森林分类器做出预测。

首先是随机森林创建阶段：

* 1.从全部“m”个特征中随机选择“K”个特征，其中k << m
* 2.在“K”个特征中，用最佳分裂点计算节点“d”
* 3.用最佳分裂将节点分裂为子节点
* 4.重复前面三步的过程，直到获得“I”个数量的节点。
* 5.重复第1到第4步“n”次创建“n”个树，从而形成一个森林。

在第二阶段，根据上一阶段创建的随机森林分类器，我们会做出预测。过程如下：

* 选取测试特征，用每个随机创建的决策树的规律去预测结果，并保存预测的结果（目标）。
* 结算每个预测目标的得票数。
* 将得票最多的预测目标作为随机森林算法的最终预测。

其实类似于使用多个决策树，数据采用bagging的方法随机，构建多个树，最后以投票的方式决定结果。

#### Sklearn
森林分类器必须拟合（fit）两个数组： 保存训练样本的数组（或稀疏或稠密的）X，大小为 [n_samples, n_features]，和 保存训练样本目标值（类标签）的数组 Y，大小为 [n_samples]:
```
>>> from sklearn.ensemble import RandomForestClassifier
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = RandomForestClassifier(n_estimators=10)
>>> clf = clf.fit(X, Y)

>>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y)
>>> scores.mean()
0.999...
```
参考文档：http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor

在极限随机树中（参见 ExtraTreesClassifier 和 ExtraTreesRegressor 类)， 计算分割点方法中的随机性进一步增强。 在随机森林中，使用的特征是候选特征的随机子集；不同于寻找最具有区分度的阈值， 这里的阈值是针对每个候选特征随机生成的，并且选择这些随机生成的阈值中的最佳者作为分割规则。 这种做法通常能够减少一点模型的方差，代价则是略微地增大偏差：
```
>>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y)
>>> scores.mean() > 0.999
True
```

使用这些方法时要调整的参数主要是 n_estimators 和 max_features。 

前者（n_estimators）是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。 此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好。 后者（max_features）是分割节点时考虑的特征的随机子集的大小。 这个值越低，方差减小得越多，但是偏差的增大也越多。 

根据经验，回归问题中使用 max_features = n_features ， 分类问题使用 max_features = sqrt（n_features （其中 n_features 是特征的个数）是比较好的默认值。 max_depth = None 和 min_samples_split = 2 结合通常会有不错的效果（即生成完全的树）。 请记住，这些（默认）值通常不是最佳的，同时还可能消耗大量的内存，最佳参数值应由交叉验证获得。 

另外，请注意，在随机森林中，默认使用自助采样法（bootstrap = True）， 然而 extra-trees 的默认策略是使用整个数据集（bootstrap = False）。 当使用自助采样法方法抽样时，泛化精度是可以通过剩余的或者袋外的样本来估算的，设置 oob_score = True 即可实现。

最后，这个模块还支持树的并行构建和预测结果的并行计算，这可以通过 n_jobs 参数实现。 如果设置 n_jobs = k ，则计算被划分为 k 个作业，并运行在机器的 k 个核上。 如果设置 n_jobs = -1 ，则使用机器的所有核。 注意由于进程间通信具有一定的开销，这里的提速并不是线性的（即，使用 k 个作业不会快 k 倍）。 当然，在建立大量的树，或者构建单个树需要相当长的时间（例如，在大数据集上）时，（通过并行化）仍然可以实现显著的加速。

参考文档：http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor

## 4.2.Boosting
### 4.2.1.原理

Boosting算法的思想是通过对弱分类器的逐步优化使之成为强分类器。

假设当前训练数据集中存在N个点，其权重分别赋值为$w_i$，在迭代学习中根据结果不断调整权重，如果分类正确，则降低这些点的权重，如果分类错误，则增加权重，这样当迭代结束时，算法将得到m个合适的模型，最后通过加权平均得到最后的预测结果。
