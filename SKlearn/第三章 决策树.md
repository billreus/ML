<!-- TOC -->

- [3.决策树](#3决策树)
    - [3.1.数学原理](#31数学原理)
        - [3.1.1.信息熵](#311信息熵)
        - [3.1.2.信息增益](#312信息增益)
        - [3.1.3.增益率](#313增益率)
        - [3.1.4.剪枝](#314剪枝)
    - [3.2.Sklearn](#32sklearn)
        - [3.2.1.分类](#321分类)
        - [3.2.2.回归](#322回归)

<!-- /TOC -->

# 3.决策树
## 3.1.数学原理
### 3.1.1.信息熵
信息熵用来表示信息的不确定性。公式如下：

$$
H\left( X \right) =-\sum_{i=1}^n{p_i\log p_i}
$$

X，表示该事件中有限个值的离散随机变量，$P_{i}$ 表示每个随机变量在整个事件中发生的概率。我信息熵计算中我们定义当 $P_{i} =0$时，$H(X)=0$，这是表示信息的不确定性最小，即事件是不发生的。

### 3.1.2.信息增益
假设离散属性a有V个可能的取值，使用a来对样本集D进行划分，则会产生V个分支节点，其中第v个分支结点包含了X中所有在属性a上取值相同的向本，记为 X^{v} 。我们算出信息熵，在考虑不同分支结点说包含的样本总数不同，给每个分支结点赋予的权重 \frac{\left| X^{v} \right|}{\left| X \right|} 也不相同，即样本数越多的分子节点的影响越大，于是可以计算出用属性a对样本X进行划分得到的信息增益。其公式如下：

$$
Gain\left( X,a \right) =H\left( X \right) -\sum_{v=1}^V{\frac{\left| X^v \right|}{\left| X \right|}H\left( X^v \right)}
$$

一般而言，信息增益越大，意味着属性a划分获得的准确率提升越大，因此ID3算法就是以信息增益来选择划分属性的。

### 3.1.3.增益率
但是ID3算法一个很大的弊端就是在属性a有很多个取值的时候算出的信息增益会很大（具体参考习惯书上的例子进行计算），这时得到的树是一个分支极多但是深度极浅的树。所以引入增益率的概念。

$$
Gain_{\gamma}atio\left( X,a \right) =\frac{Gain\left( X,a \right)}{IV\left( a \right)}
$$

其中：$IV\left( a \right) =-\sum_{v=1}^V{\frac{\left| X^v \right|}{\left| X \right|}}\log _2\frac{\left| X^v \right|}{\left| X \right|}$

C4.5算法就是先选出信息增益高于平均水平的属性，然后在选出增益率较高的属性。

### 3.1.4.剪枝
剪枝处理：

剪枝是为了防止决策树算法过拟合。决策树剪枝分为预剪枝和后剪枝。无论是预剪枝还是后剪枝都需要预留一部分数据作为验证集用于剪枝的评估。（类似于cross validation）

预剪枝：

预剪枝是在决策树生成的时候对每一个节点进行评估。在节点未生成之前根据定义该节点是一个叶节点，此时可以算出这个叶节点的准确率a，在划分节点之后再进行评估，划分后的准确率为b。如果划分前的准确率a大于划分后的准确率，则不产生这个节点。如果划分前的准确率a小于划分后的准确率b，则可以进行划分。

后剪枝：

后剪枝是在训练出一个完整的决策树之后再从最末端的节点开始往根节点依次判断是否进行剪枝。原理与预剪枝一样，根据剪枝前后验证集的准确率来判断是否需要进行剪枝。

后剪枝决策树通常比预剪枝决策树保留了更多的分支。一般情况下，后剪枝决策树过拟合风险较小，泛化能力往往优于预剪枝决策树。

## 3.2.Sklearn
### 3.2.1.分类
DecisionTreeClassifier 是能够在数据集上执行多分类的类,与其他分类器一样，DecisionTreeClassifier 采用输入两个数组：数组X，用 [n_samples, n_features] 的方式来存放训练样本。整数值数组Y，用 [n_samples] 来保存训练样本的类标签:
```
>>> from sklearn import tree
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = tree.DecisionTreeClassifier()
>>> clf = clf.fit(X, Y)
```
执行通过之后，可以使用该模型来预测样本类别:
```
>>> clf.predict([[2., 2.]])
array([1])
```
另外，也可以预测每个类的概率，这个概率是叶中相同类的训练样本的分数:
```
>>> clf.predict_proba([[2., 2.]])
array([[ 0.,  1.]])
```

参考文档：http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier

### 3.2.2.回归
决策树通过使用 DecisionTreeRegressor 类也可以用来解决回归问题。如在分类设置中，拟合方法将数组X和数组y作为参数，只有在这种情况下，y数组预期才是浮点值
```
>>> from sklearn import tree
>>> X = [[0, 0], [2, 2]]
>>> y = [0.5, 2.5]
>>> clf = tree.DecisionTreeRegressor()
>>> clf = clf.fit(X, y)
>>> clf.predict([[1, 1]])
array([ 0.5])
```

参考文档：http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor