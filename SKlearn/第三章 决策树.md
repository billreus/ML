<!-- TOC -->

- [3.决策树](#3决策树)
    - [3.1.数学原理](#31数学原理)
        - [3.1.1.信息熵](#311信息熵)
        - [3.1.2.信息增益](#312信息增益)
        - [3.1.3.增益率](#313增益率)
        - [3.1.4.CART](#314cart)
        - [3.1.5.剪枝](#315剪枝)
    - [3.2.Sklearn](#32sklearn)
        - [3.2.1.分类](#321分类)
        - [3.2.2.回归](#322回归)

<!-- /TOC -->

# 3.决策树
## 3.1.数学原理
决策树的模型可以看成是集成学习中的动态投票模型：

$$G_(x)=sign(\sum_{t=1}^T q_t(x)g_t(x))$$

* 其中$q_t(x)$表示树分支的条件，$g_t(x)$表示最后树的叶子yse or no等结果。

可以将决策树的数学模型抽象表达为：

$$G\left ( X \right)=\sum_{c=1}^C[{b(X)=c]}*G_c(X)$$

* 其中G(X)表示大树，b(X)表示分支条件，$G_c(x)$表示第c个分支下的子树。

流程可以分成四个部分：
* 首先学习设定划分不同分支的标准和条件是什么；
* 接着将整体数据集D根据分支个数C和条件，划为不同分支下的子集$D_c$；
* 然后对每个分支下的$D_c$进行训练，得到相应的机器学习模型$G_c$；
* 最后将所有分支下的$G_c$合并到一起，组成大矩G(x)

即分支个数，分支条件，终止条件，基本算法。

### 3.1.1.信息熵
信息熵用来表示信息的不确定性。公式如下：

$$H\left( X \right) =-\sum_{i=1}^n{p_i\log p_i}$$

X，表示该事件中有限个值的离散随机变量，$P_{i}$ 表示每个随机变量在整个事件中发生的概率。我信息熵计算中我们定义当 $P_{i} =0$时，$H(X)=0$，这是表示信息的不确定性最小，即事件是不发生的。

### 3.1.2.信息增益

假设离散属性a有V个可能的取值，使用a来对样本集D进行划分，则会产生V个分支节点，其中第v个分支结点包含了X中所有在属性a上取值相同的向本，记为$X^{v}$。我们算出信息熵，在考虑不同分支结点说包含的样本总数不同，给每个分支结点赋予的权重 $\frac{\left| X^{v} \right|}{\left| X \right|}$ 也不相同，即样本数越多的分子节点的影响越大，于是可以计算出用属性a对样本X进行划分得到的信息增益。其公式如下：

$$Gain\left( X,a \right) =H\left( X \right) -\sum_{v=1}^V{\frac{\left| X^v \right|}{\left| X \right|}H\left( X^v \right)}$$

一般而言，信息增益越大，意味着属性a划分获得的准确率提升越大，因此ID3算法就是以信息增益来选择划分属性的。

### 3.1.3.增益率

但是ID3算法一个很大的弊端就是在属性a有很多个取值的时候算出的信息增益会很大（具体参考习惯书上的例子进行计算），这时得到的树是一个分支极多但是深度极浅的树。所以引入增益率的概念。

$$Gain_{\gamma}atio\left( X,a \right) =\frac{Gain\left( X,a \right)}{IV\left( a \right)}$$

其中：$IV\left( a \right) =-\sum_{v=1}^V{\frac{\left| X^v \right|}{\left| X \right|}}\log _2\frac{\left| X^v \right|}{\left| X \right|}$

C4.5算法就是先选出信息增益高于平均水平的属性，然后在选出增益率较高的属性。

### 3.1.4.CART

CART有两个简单的设定，首先分支个数C=2，即二叉树数据结构。其次每个分支最后的树的叶子$g_t(x)$（YorN）是一个常数。

$g_t(x)$对于结果的选取，在分类中看正负类哪个多选哪个，回归中看平均值

如何切割数据，让数据划分的最好，即使用纯净度来衡量。

纯净度的思想就是每次切割都尽可能让左右子树中同类样本占的比例最大或者y都很接近(例如左子树全是正样本，右子树全是负样本)

$$b(x)=argmin \sum_{c=1}^2|D_c with h|*impurity(D_c with h)$$

* 其中impurity是纯净度相反的概念，$|D_c with h|$表示分支c所占权重，分支c包含样本越多权重越大。

数据集D的impurity可以用基尼值来度量(基尼值越小纯净度越高)：

$$Gini\left( D \right) =1 -\sum_{i=0}^n({\frac{D_i}{D})^2}$$

综合b(x)和上式可以得出，属性a的最优划分。

$$Gini\left( D|A \right) =\sum_{i=0}^n{\frac{D_i}{D}}Gini(Di)$$

$$a = argmin Gini\_index(D,a)$$

* 注：Di表示以A是属性值划分成n个分支里的数目

CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。

### 3.1.5.剪枝

剪枝处理：

剪枝是为了防止决策树算法过拟合。决策树剪枝分为预剪枝和后剪枝。无论是预剪枝还是后剪枝都需要预留一部分数据作为验证集用于剪枝的评估。（类似于cross validation）

预剪枝：

预剪枝是在决策树生成的时候对每一个节点进行评估。在节点未生成之前根据定义该节点是一个叶节点，此时可以算出这个叶节点的准确率a，在划分节点之后再进行评估，划分后的准确率为b。如果划分前的准确率a大于划分后的准确率，则不产生这个节点。如果划分前的准确率a小于划分后的准确率b，则可以进行划分。

后剪枝：

后剪枝是在训练出一个完整的决策树之后再从最末端的节点开始往根节点依次判断是否进行剪枝。原理与预剪枝一样，根据剪枝前后验证集的准确率来判断是否需要进行剪枝。

后剪枝决策树通常比预剪枝决策树保留了更多的分支。一般情况下，后剪枝决策树过拟合风险较小，泛化能力往往优于预剪枝决策树。

## 3.2.Sklearn

### 3.2.1.分类

DecisionTreeClassifier 是能够在数据集上执行多分类的类,与其他分类器一样，DecisionTreeClassifier 采用输入两个数组：数组X，用 [n_samples, n_features] 的方式来存放训练样本。整数值数组Y，用 [n_samples] 来保存训练样本的类标签:
```
>>> from sklearn import tree
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = tree.DecisionTreeClassifier()
>>> clf = clf.fit(X, Y)
```
执行通过之后，可以使用该模型来预测样本类别:
```
>>> clf.predict([[2., 2.]])
array([1])
```
另外，也可以预测每个类的概率，这个概率是叶中相同类的训练样本的分数:
```
>>> clf.predict_proba([[2., 2.]])
array([[ 0.,  1.]])
```

参考文档：http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier

### 3.2.2.回归

决策树通过使用 DecisionTreeRegressor 类也可以用来解决回归问题。如在分类设置中，拟合方法将数组X和数组y作为参数，只有在这种情况下，y数组预期才是浮点值

```
>>> from sklearn import tree
>>> X = [[0, 0], [2, 2]]
>>> y = [0.5, 2.5]
>>> clf = tree.DecisionTreeRegressor()
>>> clf = clf.fit(X, y)
>>> clf.predict([[1, 1]])
array([ 0.5])
```

参考文档：http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor