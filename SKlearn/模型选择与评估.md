## 交叉验证

使用交叉验证最简单的方法是在估计器和数据集上调用 cross_val_score 辅助函数。

下面的例子展示了如何通过分割数据，拟合模型和计算连续 5 次的分数（每次不同分割）来估计 linear kernel 支持向量机在 iris 数据集上的精度:

```py
>>> from sklearn.model_selection import cross_val_score
>>> clf = svm.SVC(kernel='linear', C=1)
>>> scores = cross_val_score(clf, iris.data, iris.target, cv=5)
>>> scores
array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])
```

一般数据评估常使用rmse均方根误差进行打分，首先建立一个rmse函数：

```py
from sklearn.model_selection import cross_val_score

def rmse_cv(model,X,y):
    rmse = np.sqrt(-cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=5))
    return rmse
# neg_mean_squared_error 均方误差
```

再建立需要打分调用的模型：

```py
models = [LinearRegression(),Ridge(),Lasso(alpha=0.01,max_iter=10000),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),
          ElasticNet(alpha=0.001,max_iter=10000),SGDRegressor(max_iter=1000,tol=1e-3),BayesianRidge(),KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),
          ExtraTreesRegressor(),XGBRegressor()]
```

最后将数据集代入模型打分并取均值：
```py
names = ["LR", "Ridge", "Lasso", "RF", "GBR", "SVR", "LinSVR", "Ela","SGD","Bay","Ker","Extra","Xgb"]
for name, model in zip(names, models):
    score = rmse_cv(model, train_x_sd,y_log)
    print("{}: {:.6f}, {:.4f}".format(name,score.mean(),score.std())) # format为格式化，使输出按照指定样式排列
```

### K折切分

```py
skf = StratifiedKFild(n_splits=n_splits, random_state=seed, shuffle=True) #切分数
for index,(train_index,test_index) in enumerate(skf.split(X,y)):
    X_train,X_valid,y_train,y_valid = X[train_index],X[test_index],y[train_index],y[test_index]
```

### 网格搜索（自动调参）

调参需要同时设置多个参数，对比得分,数据集默认k折验证：

```py
from sklearn.model_selection import GridSearchCV

class grid():
    def __init__(self,model):
        self.model = model

    def grid_get(self,X,y,param_grid):
        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring="neg_mean_squared_error",return_train_score=True)
        grid_search.fit(X,y)
        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))
        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])
        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])
```

调用对象时只需：

```py
grid(Ridge()).grid_get(train_x_sd,y_log,{'alpha':[10,15,20,25,30,35,40,45,50]})

out:
{'alpha': 25} 0.14007879319655245
          params  mean_test_score  std_test_score
0  {'alpha': 10}         0.141154        0.006326
1  {'alpha': 15}         0.140424        0.006278
2  {'alpha': 20}         0.140147        0.006275
3  {'alpha': 25}         0.140079        0.006289
4  {'alpha': 30}         0.140121        0.006309
5  {'alpha': 35}         0.140226        0.006330
6  {'alpha': 40}         0.140368        0.006351
7  {'alpha': 45}         0.140533        0.006371
8  {'alpha': 50}         0.140713        0.006389

```

## 打分

### F1评价函数

```py
def f1_score_vali(preds, data_vali):
    labels = data_vali.get_label()
    preds = np.argmax(preds.reshape(15, -1), axis=0)
    score_vali = f1_score(y_true=labels, y_pred=preds, average='weighted')
    return 'f1_score', score_vali, True
```
