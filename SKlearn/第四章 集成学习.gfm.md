<!-- TOC -->

- [4.集成学习](#4集成学习)
    - [4.1.Bagging（装袋）](#41bagging装袋)
        - [4.1.1.基本原理](#411基本原理)
        - [4.1.2 Sklearn](#412-sklearn)
        - [4.2.1.随机森林](#421随机森林)
            - [工作原理](#工作原理)
            - [Sklearn](#sklearn)
    - [4.2.Boosting](#42boosting)
        - [4.2.1.原理](#421原理)
        - [4.2.2.AdaBoost](#422adaboost)
        - [4.2.3.Sklearn](#423sklearn)
    - [4.3.GBDT](#43gbdt)
        - [4.3.1.原理](#431原理)
        - [4.3.2.Sklearn](#432sklearn)

<!-- /TOC -->

# 4.集成学习

集成学习可以分为三大类：
* 用于减少方差的bagging
* 用于减少偏差的boosting
* 用于提升预测结果的stacking

## 4.1.Bagging（装袋）

### 4.1.1.基本原理

每一次从原始数据中根据均匀概率分布有放回的抽取和原始数据大小相同的样本集合，样本点可能出现重复，然后对每一次产生的训练集构造一个分类器，再对分类器进行组合。

* 1.从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
* 2.每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
* 3.对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

### 4.1.2 Sklearn

在 scikit-learn 中，bagging 方法使用统一的 BaggingClassifier 元估计器（或者 BaggingRegressor ），输入的参数和随机子集抽取策略由用户指定。max_samples 和 max_features 控制着子集的大小（对于样例和特征）， bootstrap 和 bootstrap_features 控制着样例和特征的抽取是有放回还是无放回的。 当使用样本子集时，通过设置 oob_score=True ，可以使用袋外(out-of-bag)样本来评估泛化精度。

下面的代码片段说明了如何构造一个 K聚类 估计器的 bagging 集成实例，每一个基估计器都建立在 50% 的样本随机子集和 50% 的特征随机子集上。
```
>>> from sklearn.ensemble import BaggingClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> bagging = BaggingClassifier(KNeighborsClassifier(),
...                             max_samples=0.5, max_features=0.5)
```

参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier

### 4.2.1.随机森林

#### 工作原理

随机森林算法有两个阶段，一个阶段是创建随机森林，另一个是根据前一个阶段创建的随机森林分类器做出预测。

首先是随机森林创建阶段：

* 1.从全部m个特征中随机选择K个特征，其中k << m
* 2.在K个特征中，用最佳分裂点计算节点d
* 3.用最佳分裂将节点分裂为子节点
* 4.重复前面三步的过程，直到获得I个数量的节点。
* 5.重复第1到第4步n次创建n个树，从而形成一个森林。

在第二阶段，根据上一阶段创建的随机森林分类器，我们会做出预测。过程如下：

* 选取测试特征，用每个随机创建的决策树的规律去预测结果，并保存预测的结果（目标）。
* 结算每个预测目标的得票数。
* 将得票最多的预测目标作为随机森林算法的最终预测。

其实类似于使用多个决策树，数据采用bagging的方法随机，构建多个树，最后以投票的方式决定结果。

随机森林中数据集的划分采用bagging但是在和对于<img src="https://latex.codecogs.com/gif.latex?g_t(x)"/>的好坏验证上原始方法是bagging到的数据集用于得到g(x)，剩下的数据用于验证g的好坏。随机森林中采用OOB即先看每一个样本是哪些g的OOB，然后计算在这些g上的表现取平均值。

#### Sklearn

森林分类器必须拟合（fit）两个数组： 保存训练样本的数组（或稀疏或稠密的）X，大小为 [n_samples, n_features]，和 保存训练样本目标值（类标签）的数组 Y，大小为 [n_samples]:
```
>>> from sklearn.ensemble import RandomForestClassifier
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = RandomForestClassifier(n_estimators=10)
>>> clf = clf.fit(X, Y)

>>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y)
>>> scores.mean()
0.999...
```
参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor

在极限随机树中（参见 ExtraTreesClassifier 和 ExtraTreesRegressor 类)， 计算分割点方法中的随机性进一步增强。 在随机森林中，使用的特征是候选特征的随机子集；不同于寻找最具有区分度的阈值， 这里的阈值是针对每个候选特征随机生成的，并且选择这些随机生成的阈值中的最佳者作为分割规则。 这种做法通常能够减少一点模型的方差，代价则是略微地增大偏差：
```
>>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y)
>>> scores.mean() > 0.999
True
```

使用这些方法时要调整的参数主要是 n_estimators 和 max_features。 

前者（n_estimators）是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。 此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好。 后者（max_features）是分割节点时考虑的特征的随机子集的大小。 这个值越低，方差减小得越多，但是偏差的增大也越多。 

根据经验，回归问题中使用 max_features = n_features ， 分类问题使用 max_features = sqrt（n_features （其中 n_features 是特征的个数）是比较好的默认值。 max_depth = None 和 min_samples_split = 2 结合通常会有不错的效果（即生成完全的树）。 请记住，这些（默认）值通常不是最佳的，同时还可能消耗大量的内存，最佳参数值应由交叉验证获得。 

另外，请注意，在随机森林中，默认使用自助采样法（bootstrap = True）， 然而 extra-trees 的默认策略是使用整个数据集（bootstrap = False）。 当使用自助采样法方法抽样时，泛化精度是可以通过剩余的或者袋外的样本来估算的，设置 oob_score = True 即可实现。

最后，这个模块还支持树的并行构建和预测结果的并行计算，这可以通过 n_jobs 参数实现。 如果设置 n_jobs = k ，则计算被划分为 k 个作业，并运行在机器的 k 个核上。 如果设置 n_jobs = -1 ，则使用机器的所有核。 注意由于进程间通信具有一定的开销，这里的提速并不是线性的（即，使用 k 个作业不会快 k 倍）。 当然，在建立大量的树，或者构建单个树需要相当长的时间（例如，在大数据集上）时，（通过并行化）仍然可以实现显著的加速。

参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor

## 4.2.Boosting
### 4.2.1.原理

Boosting算法的思想是通过对弱分类器的逐步优化使之成为强分类器。

假设当前训练数据集中存在N个点，其权重分别赋值为<img src="https://latex.codecogs.com/gif.latex?w_i"/>，在迭代学习中根据结果不断调整权重，如果分类正确，则降低这些点的权重，如果分类错误，则增加权重，这样当迭代结束时，算法将得到m个合适的模型，最后通过加权平均得到最后的预测结果。

boosting的每一次抽样的样本分布都是不一样的。每一次迭代，都根据上一次迭代的结果，增加被错误分类的样本的权重，使得模型能在之后的迭代中更加注意到难以分类的样本，这是一个不断学习的过程，也是一个不断提升的过程，这也就是boosting思想的本质所在。迭代之后，将每次迭代的基分类器进行集成。那么如何进行样本权重的调整和分类器的集成是我们需要考虑的关键问题。

### 4.2.2.AdaBoost
设训练数据集

<img src="https://latex.codecogs.com/gif.latex?T={(x_1,y_1),(x_2,y_2)...(x_N,Y_N)}"/>

初始化训练数据的权值分布：

<img src="https://latex.codecogs.com/gif.latex?D_1=(w_{11},w_{12},...,w_{1i},...,w_{1N}),w_{1i}=\frac{1}{N}"/>

根据权重分布抽样产生数据集并学习，得到基本分类器：<img src="https://latex.codecogs.com/gif.latex?G_m(x)"/>

计算<img src="https://latex.codecogs.com/gif.latex?G_m(x)"/>在训练数据集上的分类误差率

$$
e_m=P\left( G_m\left( x_i \right) \ne y_i \right) =\sum_{i=1}^N{w_{mi}I\left( G_m\left( x_i \right) \ne y_i \right)}
$$

计算<img src="https://latex.codecogs.com/gif.latex?G_m(x)"/>的系数

$$
\alpha _m=\frac{1}{2}\log \frac{1-e_m}{e_m}
$$

根据系数开始更新数据，首先更新训练数据集的权值分布

$$
D_{m+1}=(w_{m+1,1},w_{m+1,2},...,w_{m+1,i},...,w_{m+1,N})
$$

$$
w_{m+\text{1,}j}=\frac{w_{mj}}{Z_m}\exp \left( -\alpha _my_iG\left( x_i \right) \right) ,i=\text{1,2,...,}N
$$
其中<img src="https://latex.codecogs.com/gif.latex?Z_m"/>是规范化因子，可以使D概率分布

$$
Z_m=\sum_{i=1}^N{w_{mi}\exp \left( -\alpha _my_iG_m\left( x_i \right) \right)}
$$

最后对分类器进行线性组合

$$
f\left( x \right) =\sum_{m=1}^M{\alpha _mG_m\left( x \right)}
$$

最终分类器
<img src="https://latex.codecogs.com/gif.latex?G(x)=sign(f(x))"/>

### 4.2.3.Sklearn
```
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.datasets import load_iris
>>> from sklearn.ensemble import AdaBoostClassifier

>>> iris = load_iris()
>>> clf = AdaBoostClassifier(n_estimators=100)
>>> scores = cross_val_score(clf, iris.data, iris.target)
>>> scores.mean()                             
0.9...
```
弱学习器的数量由参数 n_estimators 来控制。 learning_rate 参数用来控制每个弱学习器对 最终的结果的贡献程度（校对者注：其实应该就是控制每个弱学习器的权重修改速率，这里不太记得了，不确定）。 弱学习器默认使用决策树。不同的弱学习器可以通过参数 base_estimator 来指定。 获取一个好的预测结果主要需要调整的参数是 n_estimators 和 base_estimator 的复杂度 (例如:对于弱学习器为决策树的情况，树的深度 max_depth 或叶子节点的最小样本数 min_samples_leaf 等都是控制树的复杂度的参数)

参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier
http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor

## 4.3.GBDT
### 4.3.1.原理
GBDT是以决策树（CART）为基学习器的GB算法.

GBDT的核心就在于：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。

比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。

/*-------位置不对，后续修改-----*/
ADA算法中error为每个犯错误的样本点乘以相应的权重，求和再平均，最终得到 。

如果在决策树中使用这种方法，将当前分支下犯错误的点赋予权重，每层分支都这样做，会比较复杂，不易求解。

为了简化运算，保持决策树算法本身的稳定性和封闭性，我们可以把决策树算法当成一个黑盒子，即不改变其结构，不对算法本身进行修改，而从数据来源D上做一些处理。

可以根据权重值，对原样本集D进行一次重新的随机取样，也就是带权重的随机抽样。取样之后，会得到一个新的D。
/*---------------------------*/

### 4.3.2.Sklearn
```
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier

>>> X, y = make_hastie_10_2(random_state=0)
>>> X_train, X_test = X[:2000], X[2000:]
>>> y_train, y_test = y[:2000], y[2000:]

>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.913...
```
弱学习器(例如:回归树)的数量由参数 n_estimators 来控制；每个树的大小可以通过由参数 max_depth 设置树的深度，或者由参数 max_leaf_nodes 设置叶子节点数目来控制。 learning_rate 是一个在 (0,1] 之间的超参数，这个参数通过 shrinkage(缩减步长) 来控制过拟合。

参考文档：

http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier


## 4.4.XGBOOST
Xgboost相比于GBDT来说，更加有效应用了数值优化，最重要是对损失函数（预测值和真实值的误差）变得更复杂。目标函数依然是所有树的预测值相加等于预测值。


## 4.5.模型融合

### 4.5.1.voting

即使用投票的方式对最后结果进行投票，选票数多的为结果。
```
from sklearn.ensemble import VotingClassifier

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C=0.1,max_iter=100)

import xgboost as xgb
xgb_model = xgb.XGBClassifier(max_depth=6,min_samples_leaf=2,n_estimators=100,num_round = 5)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=200,min_samples_leaf=2,max_depth=6,oob_score=True)

from sklearn.ensemble import GradientBoostingClassifier
gbdt = GradientBoostingClassifier(learning_rate=0.1,min_samples_leaf=2,max_depth=6,n_estimators=100)

vot = VotingClassifier(estimators=[('lr', lr), ('rf', rf),('gbdt',gbdt),('xgb',xgb_model)], voting='hard')
vot.fit(train_data_X_sd,train_data_Y)
```

### 4.5.2.Stacking
Stacking是通过一个元分类器或者元回归器来整合多个分类模型或回归模型的集成学习技术。基础模型利用整个训练集做训练，元模型将基础模型的特征作为特征进行训练。

Stacking模型本质上是一种分层的结构，这里简单起见，只分析二级Stacking.假设我们有3个基模型M1、M2、M3。

